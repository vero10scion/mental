---
title: "Компьютерная работа №2"
author: "Ганин Виктор, Арзанунц Мушег, Есипова Полина, Нечаева Вероника, Утепова Ирина"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
# Предварительная работа

Установка пакетов:
```{r message=FALSE, warning=FALSE}
library('kableExtra')
library('magrittr')
library('openxlsx')
library('DescTools')
library('EnvStats')
library('outliers')
library('psych')
library('ggplot2')
library('pander')
library('fBasics')
library('nortest')
library('corrplot')
library('ppcor')
library('ggpubr')
library('questionr')
library('tseries')
library('leaps')
library('GGally')
library('lmtest')
library('summarytools')
library('dplyr')
library('lattice')
library('knitr')
library('caret')
library('FactoMineR')
library('factoextra')
library('rio')
library('REdaS')
library('biotools')
library('dbscan')
```

# Выделение главных компонент

## Вывод о числе главных компонент 

```{r}
df <- read.xlsx('Data_Psyc_Disease_Regions_absolute_values.xlsx', sheet = 'Data')
```

### Визуальный анализ распределений

Применим к данным операцию scale (простейшая нормировка данных). Это приближает эмпирическое распределение к нормальному, что позволяет использовать тесты для проверки адекватности выборки применению методов компонентного анализа.

```{r} 
df_nums <- df[,3:10]
df_normalized <- scale(df_nums)
boxplot(df_normalized, xaxt = "n", cex.lab = 0.7)
text(x =  seq_along(names(df_nums)), y = par("usr")[3] - 1, srt = 35, adj = 1,
     labels = names(df_nums), xpd = TRUE, cex = 0.6)
```

### Корреляционная матрица

Снижение размерности будет достигаться, если признаки, описывающие совокупность, достаточно коррелированы между собой. Как мы видим, практически все признаки сильно коррелированы между собой. Можем предполагать, что главных компонент будет достаточно мало.

```{r}
corrplot(cor(df_normalized), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

### Тест сферичности Бартлетта

Применим тест сферичности Бартлетта, который используется для оценки эффективности методов снижения размерности. Данный тест проверяет нулевую гипотезу о том, что теоретическая корреляционная матрица многомерного распределения вектора случайных величин, является единичной матрицей. Если нулевая гипотеза отвергается, предпосылки применения метода главных компонента будут выполнены. 

```{r}
bart_spher(df_normalized)
```

Значение p-value проведенного теста меньше уровня значимости 5%. Нулевая гипотеза отвергается, т.е. данные достаточно коррелированы, применение МГК обосновано.

### Метод главных компонент

Перейдем к самому методу главных компонент. Определим главные компоненты:

```{r}
pc <- PCA(df_normalized, graph = FALSE)
```

Собственные значения и доля суммарной вариации исходного набора признаков, приходящаяся на главные компоненты:

```{r}
pander(pc$eig)
```

Видим, что первая компонента дает вклад 76%, а первые две -- 88%. Оптимальное число определяется в соответствии с долей суммарной вариации исходных признаков, которую требуется сохранить (обычно 70-80%). В целом, можем взять в качестве оптимального количества главных компонент две. Это позволит сохранить большую долю суммарной вариации и увеличить количество главных компонент до двух. Вернемся к этому позже, когда будем строить диаграммы и графики.

```{r}
princomp <- princomp(df_normalized, cor = TRUE) 
summary(princomp)
```

Построим корреляционную матрицу для компонент и исследуемых переменных.

```{r}
pander(princomp$loadings[,1:8])
corrplot(princomp$loadings[,1:8], is.corr = FALSE, tl.col = "black")
```

Проверим линейную независимость первых двух компонент.

```{r}
cor.test(princomp$scores[,1], princomp$scores[,2])
```

Коэффициент корреляции стремится к 0.

#### Критерий Кайзера

Собственное значение компонент должно быть больше 1. 

```{r message=FALSE, warning=FALSE}
pander(pc$eig)

plot(eigen(cor(df_normalized))$values, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Собственное значение')
abline(h = 1, col = 'red', lwd = 2)
text(x = 7, y = 1.2, 'eigen value = 1', col = 'red')
```

На построенном графике видим, что оптимальное количество главных компонент 1 или 2, так как не совсем ясно, попадает ли точка на линию или лежит на ней. Благодаря проведенным ранее исследованиям, мы знаем, что собственное значение этой компоненты --- 0,97. В целом, можем предположить, что количество оптимальных компонент равняется двум.

#### Доля суммарной вариации

```{r message=FALSE, warning=FALSE}
cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')
abline(h = 50, col = 'red', lwd = 2)
abline(h = 80, col = 'blue', lwd = 2)
text(x = 8, y = 75, '70%', col = 'red')
text(x = 8, y = 81, '80%', col = 'blue')
```

Согласно доле суммарной вариации оптимальное количество главных компонент --- 1. Кумулятивное значение вариации в 70-80% достигается при использовании лишь одной --- первой компоненты.

*Примечание: линия в 70% не помещается на графике, она расположена ниже

#### Критерий каменистой осыпи

```{r}
fviz_eig(pc, addlabels = TRUE)
```

Очевидно, что оптимальное количество главных компонент --- 1. Так как именно в этой области происходит резкое падение доли сохраненной дисперсии.

#### Вывод

Фактически все проведенные тесты показали, что оптимальное количество главных компонент --- 1. Мы предполагали это еще с момента построения корреляционной матрицы: визуально было очевидно, что данные сильно коррелированны между собой.

## Описание суммарного вклада первых главных компонент


```{r}
fviz_contrib(pc, choice = "var", 
             axes = 1, fill = "steelblue", color = "steelblue",
             top = Inf)
```

Из приведенного выше графика видно,что наибольший вклад в 1 ГК вносят такие переменные как: NUM_OF_ELDERLY, NUM_OF_DIVORCES, NUM_OF_DISABLED, NUM_OF_PSYC_BEDS, NUM_OF_MEDICS,NUM_OF_DRUG_ADDICT

## Построение графика накопленного вклада ГК

График накопленного вклада главных компонент показывает, какую долю объясненной дисперсии в данных можно получить при использовании определенного количества главных компонент. Он помогает выбрать оптимальное количество главных компонент для анализа данных или оценить, насколько хорошо эти компоненты описывают структуру данных.


```{r}
# построение модели PCA и извлечение главных компонент
pca_model <- prcomp(df_normalized, center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(predict(pca_model, df_normalized))

# вычисляем объясненную дисперсию для каждой компоненты
variance <- pca_model$sdev^2
total_variance <- sum(variance)
variance_pct <- variance / total_variance * 100
cumulative_variance <- cumsum(variance_pct)

# создадим датафрейм, по которому будет строиться график
cumulative_data <- data.frame(PC = 1:length(cumulative_variance),
                              VarPct = cumulative_variance)

#само построение графиков
ggplot(cumulative_data, aes(x = PC, y = VarPct)) +
  geom_col(fill = 'red') +
  labs(x = "Главные компоненты", y = "% объясненной дисперсии") +
  ggtitle("Накопленный вклад главных компонент")

ggplot(cumulative_data, aes(x = PC, y = VarPct)) +
  geom_line() +
  labs(x = "Главные компоненты", y = "% объясненной дисперсии") +
  ggtitle("Накопленный вклад главных компонент")
```

При росте количества главных компонент доля объясненной дисперсии увеличивается, но увеличивается также и риск переобучения модели на шумовых данных. Поэтому оптимальным количеством компонент можно считать то число ГК, где доля объясненной дисперсии перестает существенно повышаться при добавлении новых компонент. На этом месте добавление новых компонент значительно не улучшает объясненную дисперсию, а только впустую увеличивает количество параметров модели. В нашем случае их будет 4, т.к. именно после 4 компоненты % объясненной дисперсии почти не изменяется и равен  приблизительно 100%

## Интерпретация главных компонент на основе анализа матрицы факторных нагрузок

Отобразим матрицу факторных нагрузок:
```{r message=FALSE, warning=FALSE}
pca <- principal(cor(df_normalized), nfactors = 1, rotate = "none", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 1, nrow = ncol(df_normalized))
rownames(pca_loadings) <- colnames(df_normalized)

kbl(round(pca_loadings, 4),
    caption = "Матрица факторных нагрузок", 
    booktabs = T, col.names = c("PC1")) %>% 
    kable_classic(html_font = "Cambria", font_size = 15, full_width = F)
```


Матрица факторных нагрузок для единственной главной компоненты показывает, как сильно каждая из переменных коррелирует с этой компонентой. Значения факторных нагрузок указывают на вклад каждой переменной в формирование главной компоненты. Чем выше значение факторной нагрузки для данной переменной, тем больший вклад она вносит в формирование главной компоненты.

При установлении порога на уровне 0,5 главные компоненты, для которых нагрузка по каждому фактору менее 0,5, считаются незначимыми. Это означает, что они не значимы для объяснения вариации данных и не могут помочь в интерпретации результата. Включение незначимых главных компонент может привести к ошибочным выводам и неправильной интерпретации, поэтому они исключаются из рассмотрения. 

В данном примере, на основе матрицы факторных нагрузок, можно сделать следующие интерпретации:

- Переменная NUM_OF_ELDERLY сильнее всего коррелирует с главной компонентой, ее факторная нагрузка наибольшая (0.9909). Это означает, что переменная NUM_OF_ELDERLY играет решающую роль в формировании главной компоненты. 

- Переменные NUM_OF_DIVORCES, NUM_OF_PSYC_BEDS и NUM_OF_DISABLED также являются важными, они имеют высокие значения факторных нагрузок (0.9854, 0.9513 и 0.9568 соответственно). 

- Переменные NUM_OF_MEDICS, NUM_OF_DRUG_ADDICT и NUM_OF_UNEMPLOY также вносят свой вклад в формирование главной компоненты, их факторные нагрузки выше нуля и превышают 0.7. 

- Переменная NUM_OF_LOWER_INCOME имеет отрицательную факторную нагрузку (-0.4349), что говорит о том, что она коррелирует обратно с главной компонентой. Это означает, что она вносит отрицательный вклад в формирование главной компоненты. 

Теперь применим варимакс-вращение:
```{r message=FALSE, warning=FALSE}
pca <- principal(cor(df_normalized), nfactors = 1, rotate = "varimax", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 1, nrow = ncol(df_normalized))
rownames(pca_loadings) <- colnames(df_normalized)

kbl(round(pca_loadings, 4),
    caption = "Матрица факторных нагрузок (с вращением варимакс)", 
    booktabs = T, col.names = c("PC1")) %>% 
    kable_classic(html_font = "Cambria", font_size = 15, full_width = F)
```

Суть варимакс-вращения заключается в выборе углов поворота m-мерной системы координат и служит для проведения содержательной интерпретации факторов, которые были выделены раннее. Как видим из полученных данных, варимакс-вращение в данном случае не требуется, так как значения остались те же.

Далее отобразим корреляционные связи между переменными и главной компонентой (отсортированы по значимости коэффициентов корреляции):

```{r}
res.desc <- dimdesc(pc, axes = 1, proba = 0.05)
res.desc$Dim.1
```

Практически у всех переменных сильная взаимосвязь с главной компонентой, а у признака NUM_OF_UNEMPLOY с ГК взаимосвязь средней силы. У всех переменных кроме NUM_OF_LOWER_INCOME положительная корреляция. Таким образом, ГК разделяет переменные на более влиятельные и менее. Изменение всех переменных в большую сторону приводит к ухудшению ситуации в регионе. Можем дать название ГК - "Социальное благополучие и здоровье населения".

```{r}
influence <- pc$ind$coord[, 1]
```

# Построение уравнения регрессии с использованием выделенных ГК

## Построение линейного уравнения регрессии на ГК

Для построения линейного уравнения регрессии на ГК необходимо удалить все выбросы из изначальной df. Мы уже делали это в первой компьютерной работе, поэтому возьмем данные оттуда.

Выведем на экран регионы, которые содержат выбросы по правилу 1IQR:
```{r}
df_nums <- df[,2:10]
lines <- c()
df_without_blowouts <- df_nums
for (cols in colnames(df_without_blowouts)) {
  outliers_1iqr <- boxplot.stats(df_without_blowouts[, cols], coef = 1)$out
  num_of_outs <- length(boxplot.stats(df_without_blowouts[, cols], coef = 1)$out)
  if (num_of_outs > 0) {
    for (i in c(1:num_of_outs)) { 
      lines <- c(lines, which( df_without_blowouts[,cols] == outliers_1iqr[i])) 
    }
  }
}
lines <- sort(lines)
lines_2 <- c() 
for (i in c(1:(length(lines) - 1))) {
  if (lines[i] != lines[i + 1]) {
    lines_2 <- c(lines_2, lines[i])
  }
}
lines_2 <- c(lines_2, lines[length(lines)])
print(df[lines_2, 1])
```

Удалим строки, соответствующие регионам, которые содержат выбросы:
```{r}
df_without_blowouts <- cbind(df[1], df_without_blowouts, df[11])
df_without_blowouts <- df_without_blowouts[-lines_2,]
```

Удалим первый и последний столбец из данных:
```{r}
df_nums_without_blowouts <- df_without_blowouts[,2:10]
```

Теперь используем функцию prcomp() на данных без выбросов, чтобы получить главную компоненту: 
```{r}
pca_nums_without_blowouts <- prcomp(df_nums_without_blowouts)
social_factor <- pca_nums_without_blowouts$x[, 1] #главная компонента
```

Далее создаем новый датафрейм с зависимой переменной NUM_OF_MENT_DIS и ГК на данных без выбросов:
```{r}
df_new <- data.frame(df_nums_without_blowouts$NUM_OF_MENT_DIS, social_factor)
colnames(df_new) <- c("NUM_OF_MENT_DIS", "social_factor")
```

В итоге строим линейную регрессию с помощью функции lm():
```{r}
lm_gk <- lm(NUM_OF_MENT_DIS ~ social_factor, data = df_new)
summary(lm_gk)
```

Показатель коэффициента детерминации достаточно высок:

$R^2_{adj}=0.6806$

Однако, тот же показатель в линейных и нелинейных моделях регрессии был выше. Рассмотрим эти моделии далее и сравним их.

По результатам F-statistic модель значима на любом разумном уровне значимости. Соответственно, полученная главная компонента значима как коэффициент регрессии. 

## Сопоставление свойств ранее полученных уравнений регресии

Для начала вcтавим необходимые нам регрессионные модели из предыдущей компьютерной работы, чтобы сравнить их с помощью AIC/BIC. Мы не учитываем двумерные модели, так как ни по одной из них не выполнились все необходимые предпосылки, то есть оптимальная модель отсутствует. 
```{r}
lm_linear_1 <- lm(NUM_OF_MENT_DIS ~ NUM_OF_PSYC_BEDS + NUM_OF_UNEMPLOY + NUM_OF_DIVORCES , data = df_nums_without_blowouts) #линейная множественной регресии

df_nums_without_blowouts[df_nums_without_blowouts == 0] <- NA
df_no_NA <- na.omit(df_nums_without_blowouts)
df_no_NA$log_NUM_OF_MENT_DIS <- log(df_no_NA$NUM_OF_MENT_DIS)
df_no_NA$log_NUM_OF_PSYC_BEDS <- log(df_no_NA$NUM_OF_PSYC_BEDS)
df_no_NA$log_NUM_OF_MEDICS <- log(df_no_NA$NUM_OF_MEDICS)
df_no_NA$log_NUM_OF_LOWER_INCOME <- log(df_no_NA$NUM_OF_LOWER_INCOME)
df_no_NA$log_NUM_OF_UNEMPLOY <- log(df_no_NA$NUM_OF_UNEMPLOY)
df_no_NA$log_NUM_OF_DRUG_ADDICT <- log(df_no_NA$NUM_OF_DRUG_ADDICT)
df_no_NA$log_NUM_OF_DISABLED <- log(df_no_NA$NUM_OF_DISABLED)
df_no_NA$log_NUM_OF_ELDERLY <- log(df_no_NA$NUM_OF_ELDERLY)
df_no_NA$log_NUM_OF_DIVORCES <- log(df_no_NA$NUM_OF_DIVORCES)

nlm_power <- lm(log(df_no_NA$NUM_OF_MENT_DIS) ~ log(df_no_NA$NUM_OF_PSYC_BEDS)  + log(df_no_NA$NUM_OF_MEDICS) + log(df_no_NA$NUM_OF_LOWER_INCOME) + log(df_no_NA$NUM_OF_UNEMPLOY) + log(df_no_NA$NUM_OF_DRUG_ADDICT) + log(df_no_NA$NUM_OF_DISABLED) + log(df_no_NA$NUM_OF_ELDERLY) + log(df_no_NA$NUM_OF_DIVORCES)) #степенная модель

IC_table <- data.frame(n = c('lm_linear_1', 'nlm_power', 'lm_gk'), 
                  a = rbind(AIC(lm_linear_1), AIC(nlm_power), AIC(lm_gk)), 
                  b = rbind(BIC(lm_linear_1), BIC(nlm_power), BIC(lm_gk)))
colnames(IC_table) <- c('Модель','Значение AIC','Значение BIC')
print(IC_table)
```

Выводы: уравнение регресии на ГК имеет большие значения информационного критерия Акаике и Байесовского информационного критерия, по величине они сопоставимы со значениями аналогичной модели множественной регресии. По сравнению со степенной моделью, определенной нами как оптимальной ранее, значения критериев в модели ГК отличаются многократно: 3,1094 (AIC степенной) против 1337,7611 (AIC ГК) и 24,2181 (BIC степенной) против 1344,1425 (BIC ГК). Так как значение критериев обратно зависит от значения функции правдоподобия, то чем меньше значение, тем лучше. Видим, что в данном случае оптимальной моделью остается прежняя $nlm\_power$.

В общем, оптимальной моделью остается степенная с коэффициентом детерминации 0,9038, остаточной стандартной ошибкой 0,2282, с критериями 3,1094 (AIC) и 24.2181 (BIC). 

# Кластерный анализ

```{r}
df_nums <- df[,3:10]
```

## Предпосылки

Применение евклидовой метрики предпочтительнее в случае слабо коррелированных признаков для кластеризации. В нашем же случае наблюдается высокая взаимозависимость между переменными (см. кор. матрицу выше), поэтому воспользуемся метрикой Махаланобиса, которая учитывает корреляции между переменными.

## Определение оптимального числа кластеров

В нашем исследовании воспользуемся тремя статистическими методами определения оптимального количества кластеров.

### Метод локтя

Данный метод основан на сравнении сумм внутрикластерных дисперсий. Считается, что оптимальное количество кластеров -- после которого убывание WSS начнет замедляться.

```{r}
fviz_nbclust(df_nums, kmeans, method = 'wss') +
  labs(x = 'Число кластеров', y = 'Сумма внутрикластерных дисперсий',
       title = 'Зависимость WSS от числа кластеров')
```

По графику видим, что уже после разделения выборки на 2 кластера убывание WSS становится менее резким, однако оно все еще существенно. О значительно более медленном убывании суммы внутрикластерных дисперсий можно говорить только после превышения границы в 3 кластера.

### Метод силуэтов

При кластеризации среднее расстояние от объекта до соседей по кластеру не должно превышать минимального среднего расстояния по прочим кластерам. Оптимальным следует считать значение параметра, соответствующее наибольшему среднему значению ширины силуэта.
```{r}
fviz_nbclust(df_nums, kmeans, method = 'silhouette') +
  labs(x = 'Число кластеров', y = 'Средняя ширина силуэта по всем точкам',
       title = 'Зависимость средней ширины силуэта от числа кластеров')
```

На графике видим, что наибольшая средня ширина силуэта -- у 2 кластеров => Данный метод позволяет считать 2 кластера оптимальным количеством для разбиения

### Статистика разрыва

Нулевая гипотеза состоит в том, что выборка взята из однородной генеральной совокупности с некоторым параметризуемым распределением.
```{r}
fviz_nbclust(df_nums, kmeans, method = 'gap_stat') +
  labs(x = 'Число кластеров', y = 'Статистика разрыва',
       title = 'Зависимость статистики разрыва от числа кластеров')
```

Графически оптимальное число кластеров согласно данному методу определяется следующим образом: число кластеров, после которого наблюдается снижение статистики разрыва следует считать оптимальным.

Таким образом, gap-статистика говорит нам о том, что разбиение на кластеры не имеет смысла => Все данные однородны, они образуют единый кластер.

### Вывод

Тем не менее, в рамках нашего исследования мы предполагаем, что субъекты Федерации делятся на, как минимум, две категории -- "С благополучной средой для поддержания ментального здоровья" и "С неблагополучной средой для поддержания ментального здоровья", поэтому попробуем разбить нашу выборку на два кластера. Помимо нашего предположения, данная гипотеза подтверждается методом силуэтов.

## Построение и анализ дендрограмм

Дендрограмма - это графическое представление результатов кластерного анализа, которое позволяет оценить близость между кластерами и определить оптимальное число кластеров. Дендрограмма состоит из вертикальных линий (дендр), которые соединяют объекты или кластеры в зависимости от расстояния между ними.

Двигаясь вверх по дереву, переменные постепенно объединяются друг с другом. Чем раньше это произойдет, тем более схожи группы наблюдений друг с другом. Те группы переменных, которые объединяются на самом верху -- отличаются друг от друга наиболее сильно. 

### Метод Варда

Метод Варда - это наиболее популярный метод иерархической кластеризации, который определяет ближайшие кластеры, объединяя их на основе минимизации дисперсии внутри каждого кластера. Из всех методов кластеризации метод Варда наиболее чувствителен к различиям в масштабе или стандартизации данных. Он также вносит наименьший вклад в феноменальное количество штрафных баллов для тех кластеров, содержание которых имеет мало общего с другими кластерами. 

```{r message=FALSE, warning=FALSE}
mahalnobis <- sqrt(D2.dist(df_nums, cov(df_nums)))

hclust_w <- hclust(mahalnobis, method = 'ward.D2')
hclust_w$labels <- df$REGION
hclust_w$cluster <- cutree(hclust_w, k = 2)
fviz_dend(hclust_w, cex = 0.5,  k = 2, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (принцип Варда)', ylab = 'Расстояние')
```

По принципу Варда произошло следующее разделение:

* В одну группу попали г. Москва и г. Санкт-Петербург

* Во вторую группу попали все остальные субъекты РФ

### Метод ближнего соседа

Метод ближнего соседа определяет расстояние между ближайшими элементами разных кластеров и образует кластеры, объединяя наименьшие расстояния. Этот метод обычно работает хорошо с группами, которые близки друг к другу. 

```{r}
hclust_nn <- hclust(mahalnobis, method = 'single')
hclust_nn$labels <- df$REGION
hclust_nn$cluster <- cutree(hclust_nn, k = 2)
fviz_dend(hclust_nn, cex = 0.5,  k = 2, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (принцип ближнего соседа)', ylab = 'Расстояние')
```

По принципу ближнего соседа:

* В отдельную категорию выделяется г. Москва

* Остальные субъекты РФ представляют собой единый кластер

### Метод дальнего соседа

Метод дальнего соседа  - метод кластеризации, основанный на нахождении максимального расстояния между элементами разных кластеров. Таким образом, два разных кластера, имеющие самые близкие элементы, не будут объединены, если между ними кластер выше предела. 

```{r}
hclust_fn <- hclust(mahalnobis, method = 'complete')
hclust_fn$labels <- df$REGION
hclust_fn$cluster <- cutree(hclust_fn, k = 2)
fviz_dend(hclust_fn, cex = 0.5,  k = 2, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (принцип дальнего соседа)', ylab = 'Расстояние')
```

По принципу дальнего соседа:

* В отдельную категорию выделяется г. Москва

* Остальные субъекты РФ представляют собой единый кластер

### Метод средней связи

Метод средней связи основывается на среднем значении расстояния между элементами одного кластера и элементами другого кластера. Этот метод представляет собой компромисс между двумя другими методами, учитывающими только самое маленькое и самое большое расстояние между кластерами, ближайшее и дальнее соответственно. 

```{r}
hclust_av <- hclust(mahalnobis, method = 'single')
hclust_av$labels <- df$REGION
hclust_av$cluster <- cutree(hclust_av, k = 2)
fviz_dend(hclust_av, cex = 0.5,  k = 2, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (принцип средней связи)', ylab = 'Расстояние')
```

По принципу средней связи:

* В отдельную категорию выделяется г. Москва

* Остальные субъекты РФ представляют собой единый кластер

## Метод взвешенного попарного среднего (WPGMA)

Метод взвешенного среднего (WPGMA) основывается на усреднении расстояния между парами элементов этих двух кластеров с учетом размера каждого кластера. Этот метод имеет тенденцию объединять кластеры, содержащие наибольшее число элементов.

```{r}
hclust_wpgma <- hclust(mahalnobis, method = 'mcquitty')
hclust_wpgma$labels <- df$REGION
hclust_wpgma$cluster <- cutree(hclust_wpgma, k = 2)
fviz_dend(hclust_wpgma, cex = 0.5,  k = 2, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (WPGMA)', ylab = 'Расстояние')
```

По принципу WPGMA:

* В отдельную категорию выделяется г. Москва

* Остальные субъекты РФ представляют собой единый кластер

## Вывод

Большая часть методов кластерного анализа говорит нам о том, что столица Российской Федерации -- г. Москва -- наиболее сильно отличается от других регионов страны по исследуемым показателям. Именно поэтому ее трудно объединить в кластер с какими-то другими субъектами. Тем не менее, метод Варда предложил немного иное разбиение -- к г. Москва добавился г. Санкт-Петербург. По всей видимости, его показатели также очень сильно отличаются от показателей по другим регионам страны.

# Использование метода k-средних для классификации объектов

## Построение и анализ графика средних показателей в кластерах

В предыдущем пункте при анализе дендрограмм мы выяснили, что 2 - это оптимальное число кластеров для наших признаков. Зафиксируем это значение в методе k-means и проведем разбиение:

```{r}
set.seed(63)
clust <- df_normalized
kmeans2 <- kmeans(clust, centers = 2)
kmeans2
```

При использовании этого метода к первому кластеру приписываются данные по 64 регионам России, ко второму - по оставшимся 21.

Построим график средних показателей в кластерах:

```{r}
plot(1:ncol(clust), kmeans2$centers[1,], type = 'l', col = 'black', lwd = 3, ylim = c(-2,3),
     ylab = 'Среднее значение признака', xlab = '', xaxt = 'n')

lines(1:ncol(clust), kmeans2$centers[2,], type = 'l', col = 'red', lwd = 3)

title('График средних показателей в кластерах')

axis(1, at=1:ncol(clust), labels = colnames(clust), las = 2, cex.axis = 0.7, adj = 0.5, srt = -45)

legend(3, 3, c('Кластер 1', 'Кластер 2'), lwd = c(3, 3), col = c('black', 'red'))

```


По горизонтали отложены классифицирующие признаки, а по вертикали – средние значения переменных для каждого кластера. На графике видно, что кластеры достаточно разнообразны, не дублируют друг друга, не пересекаются. Это говорит о хорошей кластеризации. Средние значения 2 кластера значительно больше, чем значения 1 кластера у 7 из 8 признаков. При признаке NUM_OF_LOWER_INCOME ситуация изменяется, среднее значение первого кластера становится больше второго.

## Проверка гипотезы о равенстве средних значений в кластерах

Для проверки гипотезы о равенстве средних значений в кластерах можно использовать t-критерий Стьюдента. Он проверяет нулевую гипотезу о том, что средние значения двух выборок равны, против альтернативной гипотезы о том, что средние значения выборок различаются. Стоит отметить, что использование t-критерия Стьюдента для проверки равенства средних допустимо только в том случае, когда количество кластеров не превышает двух, поэтому в нашем случае это допустимый вариант. 

```{r}
c_1 <- clust[names(kmeans2$cluster[kmeans2$cluster == 1]),]
c_2 <- clust[names(kmeans2$cluster[kmeans2$cluster == 2]),]
```

Для проверки гипотезы о равенстве средних значений по t-критерию Стьюдента, формальные условия предполагают нормальное распределение внутри каждой выборки и равенство дисперсий этих выборок. Однако, если размеры выборок достаточно большие (обычно n > 20), то распределение выборок может быть даже не нормальным, но результаты теста будут всё равно достаточно точными. В таком случае, можно пренебречь критерием нормальности 

T-критерий Стьюдента для равных дисперсий:
```{r}
t.test(c_1, c_2, var.equal = TRUE)
```

Получаем, что  p-значение меньше 0.05, поэтому мы отвергаем нулевую гипотезу о равенстве средних значений в выборках в пользу альтернативной гипотезы о различии средних значений. Разделение на 2 кластера можно считать оптимальным.

## Описание кластеров с помощью графических средств, помогающих обосновать название кластеров

Визуализируем наши два кластера с помощью функции fviz_cluster():

```{r}
fviz_cluster(object = kmeans2, data = clust,
 ellipse.type = 'convex', geom = 'point', show.clust.cent = TRUE,
 main = 'Кластеры по субъектам РФ в пространстве главных компонент')
```

Большим треугольником и кружочком выделены центры двух кластеров. Чтобы было проще выяснить характерные черты обоих кластеров, будем строить данный график в осях отдельных переменных. Начнем с NUM_OF_PSYC_BEDS  и NUM_OF_MEDICS:

```{r}
fviz_cluster(object = kmeans2, data = clust, choose.vars = c('NUM_OF_PSYC_BEDS', 
'NUM_OF_MEDICS'),
 ellipse.type = 'convex', geom = 'point',
 main = 'Кластеры субъектов РФ в пространстве NUM_OF_PSYC_BEDS и NUM_OF_MEDICS')
```

Для первого кластера характерен низкий уровень обеспеченности психиатрами и койками психиатрических специальностей, в то время как для второго -- средний и высокий.

```{r}
fviz_cluster(object = kmeans2, data = clust, choose.vars = c('NUM_OF_LOWER_INCOME', 
'NUM_OF_UNEMPLOY'),
 ellipse.type = 'convex', geom = 'point',
 main = 'Пространство NUM_OF_LOWER_INCOME и NUM_OF_UNEMPLOY')
```

В первом кластере представлены различные регионы РФ: как с очень низким, так и с достаточно высоким уровнем дохода. Количество безработных в данных субъектах на низком -- среднем уровне. Второй кластер характеризуют высокий уровень безработицы и низкий уровень доходов населения.

```{r}
fviz_cluster(object = kmeans2, data = clust, choose.vars = c('NUM_OF_DRUG_ADDICT', 
'NUM_OF_DIVORCES'),
 ellipse.type = 'convex', geom = 'point',
 main = 'Пространство NUM_OF_DRUG_ADDICT и NUM_OF_DIVORCES')
```

В первом кластере количество разводов и наркозависимых находится на сравнительно низком уровне по сравнению со вторым кластером. Во втором кластере присутствуют регионы как с малым, так и большим количеством пациентов, страдающих наркоманией.

```{r}
fviz_cluster(object = kmeans2, data = clust, choose.vars = c('NUM_OF_DISABLED', 
'NUM_OF_ELDERLY'),
 ellipse.type = 'convex', geom = 'point',
 main = 'Пространство NUM_OF_DISABLED и NUM_OF_ELDERLY')
```

Для первого кластера характерно небольшое количество как пенсионеров, так и людей с ограниченными возможностями здоровья. Во второй кластер входят регионы как с достаточно малым, так и с большим количеством инвалидов. Точно такая же ситуация и с количеством пенсионеров.

## Выводы

Весьма вероятно, что первый кластер представляет собой совокупность отдаленных регионов страны с небольшой численностью населения, с плохо развитой культурой лечения психических заболеваний. Не исключено также, что часть из них просто не диагностируется из-за слабой обеспеченности региона врачами и койками. Разводов, пенсионеров, наркозависимых и инвалидов там меньше по сравнению с остальными регионами в силу того, что и население там меньше.

Второй кластер -- в противовес первому -- это регионы страны с хорошим уровнем медицинского обеспечения, однако и с более высоким уровнем заболевания нарокманией, с большим числом пенсионеров и инвалидов. Уровень безработицы во втором кластере также выше.

В результате, на основе вышеперчисленных характеристик, дадим названия кластерам:

1) Регионы, плохо обеспеченные специалистами и койками для лечения психических расстройств, с меньшим числом факторов, способствующих развитию данных заболеваний

2) Регионы с высоким уровнем медицинского обслуживания в сфере психиатрии, с большим числом факторов, способствущих развитию ментальных расстройств

# Построение регрессионных моделей в кластерах (типологическая регрессия)

## Построение уравнений регрессии в кластерах

Присоединим к нашему датафрейму колонку с обозначением принадлежности к конкретному кластеру каждого региона:
```{r}
cl <- kmeans2$cluster
cluster_df <- as.data.frame(cbind(df, cl))
head(cluster_df)
```

Теперь разобьем cluster_df на два кластера:
```{r}
cluster_df1 <- cluster_df[kmeans2$cluster == 1,]
cluster_df2 <- cluster_df[kmeans2$cluster == 2,]   
```

Перед тем как строить уравнения регрессии необходимо определить оптимальные модели для каждого кластера. Сделаем это с помощью функции step():
```{r}
fit1 <- lm(NUM_OF_MENT_DIS ~ NUM_OF_PSYC_BEDS + NUM_OF_MEDICS + NUM_OF_LOWER_INCOME + NUM_OF_UNEMPLOY + NUM_OF_DRUG_ADDICT + NUM_OF_DISABLED + NUM_OF_ELDERLY + NUM_OF_DIVORCES, data = na.omit(cluster_df1))
fit1.step <- step(fit1, direction = 'both', trace = FALSE)
summary(fit1.step)
```

В первой модели получились 4 независимые переменные: NUM_OF_PSYC_BEDS, NUM_OF_DISABLED, NUM_OF_ELDERLY, NUM_OF_DIVORCES. 

```{r}
fit2 <- lm(NUM_OF_MENT_DIS ~ NUM_OF_PSYC_BEDS + NUM_OF_MEDICS + NUM_OF_LOWER_INCOME + NUM_OF_UNEMPLOY + NUM_OF_DRUG_ADDICT + NUM_OF_DISABLED + NUM_OF_ELDERLY + NUM_OF_DIVORCES, data = na.omit(cluster_df2))
fit2.step <- step(fit2, direction = 'both', trace = FALSE)
summary(fit2.step)
```

Во второй модели получились 2 независимые переменные: NUM_OF_UNEMPLOY и NUM_OF_DIVORCES. 

Построим уравнения регрессии для каждого из кластеров:
```{r}
reg1 <- lm(NUM_OF_MENT_DIS ~ NUM_OF_PSYC_BEDS + NUM_OF_DISABLED + NUM_OF_ELDERLY + NUM_OF_DIVORCES, data = cluster_df1)
summary(reg1)
```

```{r}
reg2 <- lm(NUM_OF_MENT_DIS ~ NUM_OF_UNEMPLOY + NUM_OF_DIVORCES, data = cluster_df2)
summary(reg2)
```
По результатам F-statistic обе модели значимы на любом разумном уровне значимости. 

Показатель коэффициента детерминации достаточно высок у обеих моделей:

$R^2_{adj}=0.881$ --- у первого кластера

Residual standard error: 5515 on 59

$R^2_{adj}=0.7317$ --- у второго кластера

Residual standard error: 19730 on 18

## Сопоставление и интерпретация коэффициентов регрессии в кластерах с использованием коэффициентов эластичности

Уравнение 1: $\ y_{mentdis} = 10.28935 + 6.58893*x_{beds} - 0.05384*x_{disabled} + 63.45741*x_{elderly} + 1.43031*x_{divorces}$

Уравнение 2: $\ y_{mentdis} = 19.540 + 343.4*x_{unemploy} + 3.538*x_{divorces}$

Посчитаем коэффициенты эластичности:

Для первого кластера:
```{r}
e_cl1_beds <- 6.58893 * (mean(cluster_df1$NUM_OF_PSYC_BEDS)/mean(cluster_df1$NUM_OF_MENT_DIS))
e_cl1_disabled <- -0.05384 * (mean(cluster_df1$NUM_OF_DISABLED)/mean(cluster_df1$NUM_OF_MENT_DIS))
e_cl1_elderly <- 63.45741 * (mean(cluster_df1$NUM_OF_ELDERLY)/mean(cluster_df1$NUM_OF_MENT_DIS))
e_cl1_divorces <- 1.43031 * (mean(cluster_df1$NUM_OF_DIVORCES)/mean(cluster_df1$NUM_OF_MENT_DIS))

e_cl1_beds
e_cl1_disabled
e_cl1_elderly
e_cl1_divorces
```

Для второго кластера:
```{r}
e_cl2_unemp <- 343.4 * (mean(cluster_df2$NUM_OF_UNEMPLOY)/mean(cluster_df2$NUM_OF_MENT_DIS))
e_cl2_divorces <- 3.538 * (mean(cluster_df2$NUM_OF_DIVORCES)/mean(cluster_df2$NUM_OF_MENT_DIS))

e_cl2_unemp
e_cl2_divorces
```
## Выводы

### Для первого кластера: 

1) Константа в начале, равная 10.28935, говорит о том, какой будет итог модели при равенстве всех влияющих факторов нулю. 

2) Отрицательное значение коэффициента перед переменной NUM_OF_DISABLED показывает обратную связь между данной переменной и NUM_OF_MENT_DIS. 

3) Коэффициенты эластичности говорят о том, что увеличение количества коек психиатрических специальностей приведет к увеличению количества душевнобольных на 22,7%, увеличение количества инвалидов уменьшит количество психически больных на 16,5%, увеличение количества пенсинеров приводит к увеличению людей с психическими заболеваниями на 72,8%, а увеличение количества разводов приведет к росту психически больных на 20,9%.

### Для второго кластера:

1) Константа в начале, равная 19.540, говорит о том, какой будет итог модели при равенстве всех влияющих факторов нулю. 

2) Увеличение количества безработных увеличит количество душевнобольных на 28,3%, а увеличение количества разводов приведет к увеличению того же показателя на 53,1%.

### Сопоставление

1) Коэффициент детерминации модели по первому кластеру больше, чем по второму (0.881 против 0,7317), что означает, что у регрессионной модели первого кластера лучшая объсняющая способность. Остаточная стандартная ошибка модели по первому кластеру в разы больше, чем в модели по второму кластеру. 

2) В первой модели участвует больше регрессоров, соответственно, влияние различных признаков на зависимую переменную можно легче отследить. 

3) У обеих моделей есть общая переменная, учавствующая в построении регрессионного уравнения --- количество разводов, это значит, что данная переменная имеет значительное влияние на зависимую переменную в обоих кластерах; значение коэффициента эластичности в двух случаях отличается --- в модели по первому кластеру оно составляет 20,9%, а во второй модели в 2,5 раза больше --- 53,1%. Это означает, что разводы в значительной степени влияют на психическое здоровье людей в регионах, относящихся ко второму кластеру, нежели к первому. Также отметим, что главными факторами увеличния числа психически больных людей в первом кластере мы можем назвать изменение количества пенсионеров, а во втором, повторимся, что определяющим фактором будет количество разводов.

## Сопоставление качества построенных моделей в кластерах и для всей совокупности объектов в целом

```{r}
lm_0 <- lm(NUM_OF_MENT_DIS ~ NUM_OF_PSYC_BEDS + NUM_OF_MEDICS + NUM_OF_LOWER_INCOME + NUM_OF_UNEMPLOY + NUM_OF_DRUG_ADDICT + NUM_OF_DISABLED + NUM_OF_ELDERLY + NUM_OF_DIVORCES, data = cluster_df)
summary(lm_0)
```

По результатам F-statistic модель значима на любом разумном уровне значимости.

Уравнение для всех объектов: 

$\ y_{mentdis} = 3225.24912 + 6.63508*x_{beds} - 9.05673*x_{medics} - 257.89122*x_{income} + 351.84719*x_{unemploy} + 12.38271*x_{addict} - 0.04892*x_{disabled} + 19.17334*x_{elderly} + 2.55244*x_{divorces}$

$R^2_{adj}=0.9198$

Residual standard error: 11630 on 76

Значение коэффициента детерминации для модели по всей совокупности в целом имеет очень большое значение. Оно превышает аналогичные значения в моделях кластеров (коэффициенты детерминации составяют 0.881 и 0.7317 соответственно). Однако значение остаточной стандартной ошибки также достаточно велико. Среди трех моделей предпочтение как наиболее оптимальной мы можем отдать модели для всей совокупности. 

## Выводы

Проведем анализ AIC/BIC:
```{r}
IC_table <- data.frame(n = c('lm_linear_1', 'nlm_power', 'lm_gk', 'reg1', 'reg2', 'lm_0'), 
                  a = rbind(AIC(lm_linear_1), AIC(nlm_power), AIC(lm_gk), AIC(reg1), AIC(reg2), AIC(lm_0)), 
                  b = rbind(BIC(lm_linear_1), BIC(nlm_power), BIC(lm_gk), BIC(reg1), BIC(reg2), BIC(lm_0)))
colnames(IC_table) <- c('Модель','Значение AIC','Значение BIC')
print(IC_table)
```

Как мы видим, регрессионные модели на кластеры не дали наиболее оптимальные результаты. Несмотря на превосходство значения коэффициента детерминации над всеми остальными коэффициентами иных моделей, результат AIC/BIC ясно дает понять, что оптимальной остается степенная регрессионная модель. Интересно отметить, что наибольшее значение по данным тестам у модели в кластерах для всей совокупности объектов, несмотря на наибольшее значение коэффициента детерминации.

# Дискриминантный анализ

## Построение дискриминантных функций

Рассмотрим корреляционную матрицу наших признаков-предикторов:
```{r}
corrplot(cor(df_normalized), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

Почти все они имеют сильную корреляцию друг с другом. Избавимся от мультиколлениарности, убрав некоторые из них:
```{r}
df_no_multy <- df_normalized[,-c(2, 6:8)]
corrplot(cor(df_no_multy), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)

```

В предыдущей части работы мы произвели кластеризацию методом k-means. В результате 64 региона России были отнесены к первому кластеру, а оставшиеся 21 - ко второму. Присоединим к нашему датафрейму колонку с обозначением принадлежности к конкретному кластеру каждого региона:

```{r}
cl <- kmeans2$cluster
cluster_df <- as.data.frame(cbind(df_no_multy, cl))
head(cluster_df)
```

Сделаем разделение на обучающую и тестовую выборки:
```{r}
smpl_size <- floor(2/3 * nrow(cluster_df))

set.seed(11)
train_ind <- sample(seq_len(nrow(cluster_df)), size = smpl_size)

data.train <- as.data.frame(cluster_df[train_ind,])
data.unknown <- as.data.frame(cluster_df[-train_ind,])

# Сохраним также данные с названиями регионов, чтобы далее было удобнее строить дендрограммы
data.train_with_labels <- as.data.frame(df[train_ind,])
data.unknown_with_labels <- as.data.frame(df[-train_ind,])
```

Построим дискриминантную функцию:
```{r, warning = FALSE, message = FALSE}
lda.fit <- lda(data.train[,-c(5)], data.train$cl)
lda.fit
```
Беря во внимание что соотношение регионов по k-means было 3:1 в пользу первого кластера, становится понятно, почему функция вывела априорные вероятности принадлежности к группам как 0,71 - к первой и 0,29 - ко второй.

Коэффициенты линейного дискриминанта (LD1) для каждого признака-предиктора:
-0.037 * NUM_OF_PSYC_BEDS  - 0.149 * NUM_OF_LOWER_INCOME * 1.127 * NUM_OF_UNEMPLOY * 1.035 * NUM_OF_DRUG_ADDICT

Визуализируем дискриминантную функцию:
```{r}
plot(lda.fit)
```
## Выводы о качестве модели

Для оценки качества модели воспользуемся Лямбдой Уилкса:
```{r}
lda.pred <- predict(lda.fit, data.unknown[,-c(5)])

ldam <- manova(as.matrix(data.unknown[,-c(5)]) ~ lda.pred$class)

summary(ldam, test = "Wilks")
```
 
Лямбда Уилкса  показывает, значимо ли различаются средние в кластерах. Наше значение равно 0.218, что больше 0.05(уровня значимости), т.е. средние значения различаются незначительно. Превышение порога значимости в 4 раза делает нашу модель далекой от идеала, но <b>приемлемой</b> для использования.

## Отнесение новых объектов

## 1 способ 

Данный способ выполняет кластеризацию нового наблюдения (`new_data_new`) на основе предварительно обученной модели K-means. Функция находит ближайший центр кластера для каждого нового наблюдения, используя функцию `apply` и анонимную функцию. Затем категориальный вектор значений создается с помощью функции `as.factor`. На выходе получаем вектор значений кластеров для каждого нового наблюдения.

```{r}
set.seed(63)
clust <- df_nums
kmeans_cluster <- kmeans(clust, centers = 2)
```

```{r}
new_data = data.frame(NUM_OF_PSYC_BEDS = 1, NUM_OF_LOWER_INCOME = 12, NUM_OF_DRUG_ADDICT = 0.075, NUM_OF_UNEMPLOY = 13)
```
 
```{r warning=FALSE}
new_data$kmeans_cluster_1 <- as.factor(
  apply(new_data, 1, function(x) {
    which.min(sqrt(rowSums((x - kmeans_cluster$centers)^2)))
  })
)
new_data$kmeans_cluster_1
```
Сгенерированное наблюдение (субъект) с числом коек психиатрических специальностей, равным 1, с 12% населения с денежными доходами ниже границы бедности, с 0,075 пациентами с синдромом зависимости от наркотических веществ и с 13 тысячами безработных в возрасте 15-72 лет был отнесен к 1-му кластеру, плохо обеспеченному врачами и койками психиатрических специальностей, однако с меньшим числом предпосылок для возникновения ментальных расстройств. 

Попробуем изменить значения признаков и проанализировать получившиеся результаты:
```{r}
new_data_new = data.frame(NUM_OF_PSYC_BEDS = 1.5, NUM_OF_LOWER_INCOME = 11, NUM_OF_DRUG_ADDICT = 0.062, NUM_OF_UNEMPLOY = 15)
```

```{r warning=FALSE}
new_data_new$kmeans_cluster_2 <- as.factor(
  apply(new_data_new, 1, function(x) {
    which.min(sqrt(rowSums((x - kmeans_cluster$centers)^2)))
  })
)
new_data_new$kmeans_cluster_2
```


```{r}
new_data_new_3 = data.frame(NUM_OF_PSYC_BEDS = 1.43, NUM_OF_LOWER_INCOME = 11, NUM_OF_DRUG_ADDICT = 0.02, NUM_OF_UNEMPLOY = 14.42)
```

```{r warning=FALSE}
new_data_new_3$kmeans_cluster_3 <- as.factor(
  apply(new_data_new_3, 1, function(x) {
    which.min(sqrt(rowSums((x - kmeans_cluster$centers)^2)))
  })
)
new_data_new_3$kmeans_cluster_3
```
Как мы видим, последующие объекты также были отнесены к первому кластеру.

## 2 способ

Проанализируем результаты отнесения новых объектов, используя другой способ. 

Данный метод позволяет использовать предварительно обученную модель LDA для прогнозирования принадлежности новых наблюдений к определенным группам на основе известных переменных.

```{r}
lda.pred_1 <-predict(lda.fit, newdata = data.frame(NUM_OF_PSYC_BEDS = 1.4, NUM_OF_LOWER_INCOME = 18, NUM_OF_UNEMPLOY = 18.8, NUM_OF_DRUG_ADDICT = 0.07))
summary(lda.pred_1)
```

```{r}
lda.pred_2 <-predict(lda.fit, newdata = data.frame(NUM_OF_PSYC_BEDS = 1.46, NUM_OF_LOWER_INCOME = 13.5, NUM_OF_UNEMPLOY = 25, NUM_OF_DRUG_ADDICT = 0.12))
summary(lda.pred_2)
```

```{r}
lda.pred_3 <-predict(lda.fit, newdata = data.frame(NUM_OF_PSYC_BEDS = 2, NUM_OF_LOWER_INCOME = 12, NUM_OF_UNEMPLOY = 18.5, NUM_OF_DRUG_ADDICT = 0.05))
summary(lda.pred_3)
```
Все три наблюдения были отнесены к первому кластеру, это значит, что они содержат значения переменных, близкие к тем, которые характеризуют первую группу в обучающей выборке, которую и классифицировала модель LDA. Кроме того, значение функций дискриминантных анализа и их произведения в объектах `lda.pred_1`, `lda.pred_2` и `lda.pred_3`, которое выводится с помощью функции `summary`, могут использоваться для более детального анализа причин, определяющих принадлежность нового наблюдения к тому или иному кластеру.


## Уточнение результатов классификации, выполненной с помощью метода к-средних
```{r}
plot(lda.fit)
```

Можно заметить, что наблюдения не из одной группы распределились на разных участках.

```{r}
ldahist(data = lda.pred$x[,1], g = cl)
```

Анализируя гистограмму значений первой дискриминантной функции, мы можем заметить очень явную разницу между значениями этой функции для разных групп данных. Это может служить доказательством того, что выполненная нами кластеризация с помощью метода к-средних была довольно эффективной и результаты, полученные на ее основе, могут быть использованы.


## Анализ классификационной матрицы

```{r}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.unknown[,c("cl")])
```

По таблице соответствия предсказанных классов исходным (столбцы -- реальные значения, строки -- предсказанные) видим, что в данном случае дискриминантная функция сработала безошибочно. 24 объекта, принадлежащих к первому кластеру, были верно отнесены к нему, 5 объектов, относящихся ко второму кластеру, были распределены ко 2 кластеру. Уровень ошибок по каждому из кластеров, да и по модели в целом, равен 0.

ДИскриминантная функция отлично справилась со своей задачей.

## Построение графика принадлежности тестовой и тренировочной выборок к кластерам

Построим дендрограммы, используя принцип Варда:
```{r}
df_tr <- data.train[,-5]
mahalanobis_1 <- sqrt(D2.dist(df_tr, cov(df_tr)))
hclust_w1 <- hcut(mahalanobis_1, hc_method =
'ward.D2')
hclust_w1$labels <- data.train_with_labels$REGION
fviz_dend(hclust_w1, 
cex = 0.6, 
color_labels_by_k = TRUE, 
main = 'Дендрограмма, расстояние Махаланобиса (принцип Варда)', ylab = 'Расстояние') 
```

**На рис. -- обучающая выборка**

```{r}
df_un <- data.unknown[,-5]
mahalanobis_2 <- sqrt(D2.dist(df_un, cov(df_un)))
hclust_w2 <- hcut(mahalanobis_2, hc_method =
'ward.D2')
hclust_w2$labels <- data.unknown_with_labels$REGION
fviz_dend(hclust_w2, 
cex = 0.6, 
color_labels_by_k = TRUE, 
main = 'Дендрограмма, расстояние Махаланобиса (принцип Варда)', ylab = 'Расстояние') 
```

**На рис. -- тестовая выборка**

Интерпретация полученных дендрограмм: Поскольку на обоих графиках сохраняется одинаковое количество кластеров (параметр k намеренно не был заранее задан), нельзя сказать, что модель обладает плохой обучающей способностью. Общая структура построения дендрограмм похожа, что также говорит о том, что полученная нами дискриминантная функция имеет довольно неплохую предсказательную способность

Также можем посмотреть на наши кластеры в пространстве главных компонент:
```{r}
complete_train <- data.train[complete.cases(data.train), ]
cluster_train <- scale(complete_train)
kmeans_train <- kmeans(cluster_train, centers = 2)
fviz_cluster(object = kmeans_train, data = data.train,
 ellipse.type = 'convex', geom = 'point',
 main = 'Обучающая выборка в пространстве главных компонент')
```


```{r}
complete_test <- data.unknown[complete.cases(data.unknown), ]
cluster_test <- scale(complete_test)
kmeans_test <- kmeans(cluster_test, centers = 2)
fviz_cluster(object = kmeans_test, data = data.unknown,
 ellipse.type = 'convex', geom = 'point',
 main = 'Тестовая выборка в пространстве главных компонент')
```

Интерпретация кластеров в пространстве главных компонент: Кластеры имеют сходное положение, поэтому можно говорить о достаточном уровне надежности построенной модели. Однако, их форма отличается, вероятнее всего, из-за того, что соотношение между обучающей и тестовой выборками было 2 к 1. 

